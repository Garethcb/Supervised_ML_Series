{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SoftMax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In instances that we require more than two classes we can use multinomial logistic regression aka SoftMax regression.\n",
    "- Unlike logistic regression where y is binary: $y_i\\in \\{0,1\\}$\n",
    "- In multinomial logistic regression our target variable y ranges over a number of classes: $y_i\\in \\{1,2,..,k\\}$\n",
    "  \n",
    "Our goal is to estimate the probability of y being in each potential class (i.e taking on each of the K different values): \n",
    "  \n",
    "$$P(y=k|x)$$\n",
    "\n",
    "Therefore, given our indepeendent variables x, we want our hypothesis to estimate the probability that $y^{(i)}$ is a member of the class given $x_i$ namely, $P(y=k|x)$ for each value of $k=1,…,K$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization of Logistic Regression (Sigmoid Function)\n",
    "The multinomial logistic classifier uses a generalization of the sigmoid, called\n",
    "the softmax function, to compute $p(y = k|x)$.\n",
    "- Sigmoid function: $$\\frac{1}{1+e^{-z}}$$\n",
    "  \n",
    "- Softmax function: $$\\frac{e^{z_i}}{\\sum_{i=1}^{m}e^{z_j}}$$\n",
    "  \n",
    "- Both functions take z as an input: $$z = \\sum_{i=1}^{m} w_ix_i = {\\theta^Tx}$$\n",
    "  \n",
    "  \n",
    "- **SoftMax regression** estimates the probability of y being in **each potential K**, contrastly, Logistic regession uses a threshold function to convert the predicted probability into a binary outcome:\n",
    "\n",
    "$$ \\hat{y} =\n",
    "  \\begin{cases}\n",
    "    1       & \\quad \\text{if } \\hat{y} \\geq 0.5\\\\\n",
    "    0  & \\quad \\text{if } \\hat{y} < 0.5\n",
    "  \\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "$$h_\\theta (x) = \n",
    " \\begin{pmatrix}\n",
    "  P(y = 1|x;\\theta) \\\\\n",
    "  P(y = 2|x;\\theta) \\\\\n",
    "  \\vdots\\\\\n",
    "  P(y = k|x;\\theta)\n",
    " \\end{pmatrix} = \\frac{1}{\\sum_{j=1}^{K}exp^{\\theta_j^Tx}} \\begin{pmatrix}\n",
    "  exp^{\\theta_1^Tx} \\\\\n",
    "  exp^{\\theta_2^Tx} \\\\\n",
    "  \\vdots\\\\\n",
    "  exp^{\\theta_k^Tx}\n",
    " \\end{pmatrix} = \\frac{exp^{(\\theta_k^Tx)}}{\\sum_{j=1}^{K}exp^{(\\theta_j^Tx)}}$$\n",
    "\n",
    "Notably, the term $\\frac{1}{\\sum_{i=1}^{m}e^{z_j}}$ normalizes the distribution so that it sums to one. This allows the \n",
    "softmax function to take a vector $z = [z1,z2,...,zk]$ of k arbitrary values and map them into a probability \n",
    "distribution $[0,1]$. \n",
    "\n",
    "We can express the probability that $y^{(i)}$ is equal to each class given $x^{(i)}$ parameterised by $\\theta$ as follows:\n",
    "\n",
    "$$P(y^{(i)}=k|x^{(i)};\\theta) =  \\frac{exp^{\\theta_k^Tx^{(i)}}}{\\sum_{j=1}^{K}exp^{\\theta_j^Tx^{(i)}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function\n",
    "\n",
    "The loss function for multinomial logistic regression is a generalization of the loss function for logistic regression from 2 to K classes\n",
    "\n",
    "$$LogRegression = J(w)= \\sum_{i=1}^{m}-[y^{(i)}log(\\hat{y}^{(i)})+(1-y)log(1-\\hat{y}^{(i)})]$$\n",
    "\n",
    "Cross-Entropy loss: \n",
    "$$L_{CE}(\\hat{y},y) = -\\sum_{k=1}^{K}y_klog\\hat{y}_k = -\\sum_{k=1}^{K}y_klog(y=k|x;\\theta)$$\n",
    "\n",
    "\n",
    "\n",
    "Softmax Regression generalizes the two terms in the above equation. The loss function for a single example x is thus the sum of the logs of the K\n",
    "output classes, each weighted by $y_k$\n",
    ", the probability of the true class.\n",
    "\n",
    "###### One Hot Encoded Vector\n",
    "\n",
    "Notably, only one class is the correct one so the vector y takes the value\n",
    "1 only for this value of k and the rest take the value 0:\n",
    "\n",
    "$$OHE = \n",
    " \\begin{pmatrix}\n",
    "  0 \\\\\n",
    "  1\\\\\n",
    "  \\vdots\\\\\n",
    "  0\n",
    " \\end{pmatrix}$$\n",
    " \n",
    "For example, if we are predicting cat then only $y_2$ is the correctly labeled instance. This means the terms in the  loss function above will be 0 except for the term corresponding to the true class $y_2$.\n",
    "\n",
    "$$L_{CE}(\\hat{y},y) = -\\sum_{k=1}^{K}y_2log\\hat{y}_2 = -\\sum_{k=1}^{K}log\\hat{y}_2$$\n",
    "\n",
    "Where $y_2$ becomes 1 so we can just drop it and $\\hat{y} = P(y=k|x;\\theta) =  \\frac{exp^{\\theta_k^Tx}}{\\sum_{j=1}^{K}exp^{\\theta_j^Tx}}$\n",
    "\n",
    "We can therefore write our cost function as the negative log likelihood loss:\n",
    "\n",
    "$$L_{CE}(\\hat{y},y) =-\\sum_{k=1}^{K}\\mathbb{1}\\{y=k\\}log\\hat{p}(y=k|x;\\theta)$$\n",
    "\n",
    "$$J(\\theta) =-\\sum_{k=1}^{K}\\mathbb{1}\\{y=k\\}log\\frac{exp^{\\theta_k^Tx}}{\\sum_{j=1}^{K}exp^{\\theta_j^Tx}}$$\n",
    "\n",
    "\n",
    "So when we are dealing with the correctly labeled instance $\\mathbb{1}\\{y^{(i)}=k\\} = 1$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding\n",
    "def One_Hot(y):\n",
    "    Unique,inverse = np.unique(y, return_inverse=True)\n",
    "    return np.eye(Unique.shape[0])[inverse]\n",
    "#One_Hot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "There is no closed form solution to find the minimum of J(θ). Therefore, we will use an iterative optimization algorithm:\n",
    "\n",
    "$$\\nabla_{\\theta^{k}}J(\\theta) = - \\sum_{i=1}^{m}[x^{(i)}(\\mathbb{1}\\{y^{(i)}=k\\} - P(y^{(i)}=k|x^{(i)};\\theta))]$$\n",
    "\n",
    "$$\\nabla_{\\theta^{k}}J(\\theta) = - \\sum_{i=1}^{m}[x^{(i)}(\\mathbb{1}\\{y^{(i)}=k\\} - \\frac{exp^{\\theta_k^Tx}}{\\sum_{j=1}^{K}exp^{\\theta_j^Tx}})]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sns.load_dataset('iris')\n",
    "df = pd.DataFrame(iris)\n",
    "X = df.drop('species',axis=1)\n",
    "y = df['species']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMaxRegression():\n",
    "    def __init__ (self,n_epochs=50,learning_rate=0.1,tol= 1e-4,penalty =0):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tol = tol\n",
    "        self.penalty = penalty\n",
    "\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        X = np.c_[np.ones((len(X))),X]\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        #convergence check\n",
    "        previous_loss = -float('inf')\n",
    "        self.converged = False\n",
    "\n",
    "        y_mult = self.One_Hot(y)\n",
    "        weight = np.zeros([X.shape[1],len(np.unique(y))])\n",
    "        momentum = weight * 0\n",
    "        self.loss_history = []\n",
    "        self.y_mult = y_mult\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            for i in range(m):\n",
    "                random_index = np.random.randint(m)\n",
    "                xi = X[random_index:random_index+1]\n",
    "                yi = y_mult[random_index:random_index+1]\n",
    "                scores = np.dot(xi,weight)\n",
    "                softmax_prob = self.softmax(scores)\n",
    "                loss = (-1 / m) * np.sum(yi * np.log(softmax_prob)) + (self.penalty/2)*np.sum(np.square(weight))\n",
    "                self.loss_history.append(loss)\n",
    "\n",
    "                gradient = (-1/m) * np.dot(xi.T,(yi-softmax_prob))\n",
    "                gradient[1:] += (self.penalty/m) * weight[1:]\n",
    "                self.learning_rate = self.learning_schedule(epoch * m + i)\n",
    "                momentum = 0.8 * momentum + self.learning_rate * gradient\n",
    "                weight -= momentum\n",
    "\n",
    "        self.weight = weight\n",
    "        return self\n",
    "    \n",
    "    def predict(self,someX):\n",
    "        someX = np.c_[np.ones((len(someX))),someX]\n",
    "        probs = self.softmax(np.dot(someX,self.weight))\n",
    "        preds = np.argmax(probs,axis=1)\n",
    "        self.proability = probs\n",
    "        self.prediction = preds\n",
    "        return probs,preds\n",
    "    \n",
    "    def accuracy(self,Y_test,X_test):\n",
    "        Y_test =  np.unique(Y_test, return_inverse=True)[1]\n",
    "        accuracy = np.sum(X_test[1] == Y_test)/(float(len(Y_test)))\n",
    "        return accuracy\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        z -= np.max(z)\n",
    "        return (np.exp(z).T/np.sum(np.exp(z),axis=1)).T\n",
    "    \n",
    "    def One_Hot(self, y):\n",
    "        Unique,inverse = np.unique(y, return_inverse=True)\n",
    "        self.ohe = np.eye(Unique.shape[0])[inverse]\n",
    "        return self.ohe\n",
    "    \n",
    "    def sigmoid(sef,z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def learning_schedule(self,t,t0 =5, t1 = 50):\n",
    "        return t0 / (t + t1)\n",
    "        \n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7666666666666667"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softreg = SoftMaxRegression().fit(X_train,y_train)\n",
    "y_pr = softreg.predict(X_test)\n",
    "softreg.accuracy(y_test,y_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
