{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# <center> SoftMax Regression\n",
    "\n",
    "<center> In instances that we require more than two classes we can use multinomial logistic regression aka SoftMax regression.\n",
    "    \n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center>  Unlike logistic regression where y is binary: \n",
    "\n",
    "$$y_i\\in \\{0,1\\}$$\n",
    "___\n",
    "#### <center> In multinomial logistic regression our target variable y ranges over a number of classes: \n",
    "    \n",
    "$$y_i\\in \\{1,2,..,k\\}$$\n",
    "\n",
    "___\n",
    "\n",
    "#### <center> Our goal is to estimate the probability of y being in each potential class (i.e taking on each of the K different values): \n",
    "\n",
    "$$P(y=k|x)$$\n",
    "___\n",
    "#### <center> We want our hypothesis to estimate the probability $P$ that our observation $y^{(i)}$ is a member of the class given $x_i$ \n",
    "    \n",
    "$$P(y=k|x)$$ \n",
    " \n",
    "$$\\text{ For each value of }$$\n",
    "    \n",
    "$$k=1,…,K$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## <center> Generalization of Logistic Regression (Sigmoid Function)\n",
    "<center> The multinomial logistic classifier uses a generalization of the sigmoid, called the softmax function, to compute $p(y = k|x)$.\n",
    "    \n",
    "___\n",
    "\n",
    "#### <center> Sigmoid function: \n",
    "    \n",
    "$$\\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "___\n",
    "#### <center> Softmax function: \n",
    "$$\\frac{e^{z_i}}{\\sum_{i=1}^{m}e^{z_j}}$$\n",
    "___\n",
    "#### <center> Both functions take z as an input: \n",
    "$$z = \\sum_{i=1}^{m} w_ix_i = {\\theta^Tx}$$\n",
    "___\n",
    "  \n",
    "#### <center> Logistic regession uses a threshold function to convert the predicted probability into a binary outcome:\n",
    "\n",
    "$$ \\hat{y} =\n",
    "  \\begin{cases}\n",
    "    1       & \\quad \\text{if } \\hat{y} \\geq 0.5\\\\\n",
    "    0  & \\quad \\text{if } \\hat{y} < 0.5\n",
    "  \\end{cases}\n",
    "$$\n",
    "___\n",
    "    \n",
    "#### <center> Contrastly, **SoftMax regression** estimates the probability of y being in **each potential K** (We would predict cat below)\n",
    "    \n",
    "$$\\begin{pmatrix}\n",
    "  0.9 \\\\\n",
    "  0.05 \\\\\n",
    "  0.02\\\\\n",
    "  0.03\n",
    " \\end{pmatrix} = \\begin{pmatrix}\n",
    "  Cat \\\\\n",
    "  Dog \\\\\n",
    "  Cow\\\\\n",
    "  Mouse\n",
    " \\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## <center> Hypothesis\n",
    "___\n",
    "$$h_\\theta (x) = \n",
    " \\begin{pmatrix}\n",
    "  P(y = 1|x;\\theta) \\\\\n",
    "  P(y = 2|x;\\theta) \\\\\n",
    "  \\vdots\\\\\n",
    "  P(y = k|x;\\theta)\n",
    " \\end{pmatrix} = \\frac{1}{\\sum_{j=1}^{K}exp^{\\theta_j^Tx}} \\begin{pmatrix}\n",
    "  exp^{\\theta_1^Tx} \\\\\n",
    "  exp^{\\theta_2^Tx} \\\\\n",
    "  \\vdots\\\\\n",
    "  exp^{\\theta_k^Tx}\n",
    " \\end{pmatrix} = \\frac{exp^{(\\theta_k^Tx)}}{\\sum_{j=1}^{K}exp^{(\\theta_j^Tx)}}$$\n",
    "___\n",
    "### <center> Normalised \n",
    "Notably, the term $\\frac{1}{\\sum_{i=1}^{m}e^{z_j}}$ normalizes the distribution so that it sums to one. This allows the \n",
    "softmax function to take a vector $z = [z1,z2,...,zk]$ of k arbitrary values and map them into a probability \n",
    "distribution $[0,1]$. \n",
    "___\n",
    "#### <center> We can express the probability that $y^{(i)}$ is equal to each class given $x^{(i)}$ parameterised by $\\theta$ as follows:\n",
    "\n",
    "$$P(y^{(i)}=k|x^{(i)};\\theta) =  \\frac{exp^{\\theta_k^Tx^{(i)}}}{\\sum_{j=1}^{K}exp^{\\theta_j^Tx^{(i)}}}$$\n",
    "___\n",
    "    \n",
    "## <center> Denoting our Predictions\n",
    "\n",
    "#### <center> Logistic regression:\n",
    "    \n",
    "$$\\hat{y} = \\sigma(\\theta^Tx)$$\n",
    "    \n",
    "___   \n",
    "#### <center> Softmax regression:\n",
    "    \n",
    "$$\\hat{y} = \\frac{exp^{\\theta_k^Tx^{(i)}}}{\\sum_{j=1}^{K}exp^{\\theta_j^Tx^{(i)}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# <center> Cost Function\n",
    "___\n",
    "\n",
    "#### <center> The loss function for multinomial logistic regression is a generalization of the loss function for logistic regression from 2 to K classes\n",
    "\n",
    "$$\\text{Logistic Regression} = J(w)= \\sum_{i=1}^{m}-[y^{(i)}log(\\hat{y}^{(i)})+(1-y)log(1-\\hat{y}^{(i)})]$$\n",
    "___\n",
    "    \n",
    "#### <center> Cross-Entropy loss: \n",
    "    \n",
    "$$L_{CE}(\\hat{y},y) = -\\sum_{k=1}^{K}y_klog\\hat{y}_k = -\\sum_{k=1}^{K}y_klog(y=k|x;\\theta)$$\n",
    "\n",
    "\n",
    "$$$$\n",
    "<center> Softmax Regression generalizes the two terms in the above equation. The loss function for a single example x is thus the sum of the logs of the K output classes, each weighted by $y_k$, the probability of the true class.\n",
    "    \n",
    "___\n",
    "## <center>  One Hot Encoded Vector\n",
    "\n",
    "#### <center> Notably, only one class is the correct one so the vector y takes the value 1 only for this value of k and the rest take the value 0:\n",
    "\n",
    "$$OHE = \n",
    " \\begin{pmatrix}\n",
    "  0 \\\\\n",
    "  1\\\\\n",
    "  \\vdots\\\\\n",
    "  0\n",
    " \\end{pmatrix}$$\n",
    "___\n",
    "For example, if we are predicting cat then only $y_2$ is the correctly labeled instance. This means the terms in the  loss function above will be 0 except for the term corresponding to the true class $y_2$.\n",
    "\n",
    "$$L_{CE}(\\hat{y},y) = -\\sum_{k=1}^{K}y_2log\\hat{y}_2 = -\\sum_{k=1}^{K}log\\hat{y}_2$$\n",
    "___\n",
    "#### <center> Where $y_2$ becomes 1 so we can just drop it\n",
    "$$\\hat{y} = P(y=k|x;\\theta) =  \\frac{exp^{\\theta_k^Tx}}{\\sum_{j=1}^{K}exp^{\\theta_j^Tx}}$$\n",
    "    \n",
    "___\n",
    "#### <center> We can therefore write our cost function as the negative log likelihood loss:\n",
    "\n",
    "$$L_{CE}(\\hat{y},y) =-\\sum_{k=1}^{K}\\mathbb{1}\\{y=k\\}log\\hat{p}(y=k|x;\\theta)$$\n",
    "$$$$\n",
    "$$J(\\theta) =-\\sum_{k=1}^{K}\\mathbb{1}\\{y=k\\}log\\frac{exp^{\\theta_k^Tx}}{\\sum_{j=1}^{K}exp^{\\theta_j^Tx}}$$\n",
    "\n",
    "___\n",
    "So when we are dealing with the correctly labeled instance $\\mathbb{1}\\{y^{(i)}=k\\} = 1$.\n",
    "\n",
    "Incorrectly labeled instance $\\mathbb{1}\\{y^{(i)}=k\\} = 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# <center> Derivatives\n",
    "___\n",
    "___\n",
    "1. For optimization problems, Machine Learning alogrithyms utilize derivatives. Most notably, Optimization algorithms like gradient descent utilize derivates to decide whether to increase or decrease our feature weights $\\theta$ in order to march towards our cost functions minima or maxima. \n",
    "    \n",
    "    \n",
    "2. Therefore, If we are able to compute the derivative of our specific function, this information tells us in which direction to proceed in order to minimize that function.\n",
    "    \n",
    "___    \n",
    "Important notes:\n",
    "   - The Derivative tells us the slope of a function at any point.\n",
    "    \n",
    "    \n",
    "   - We know from single variable calculus that taking the derivative of a function and setting that derivative equal to zero provides us with that functions critical points.\n",
    "    \n",
    "    \n",
    "   - The second derivative provides us with the functions inflection points.\n",
    "    \n",
    "    \n",
    "   - The second derivative test tells us if our critical points are minima, maxima, or saddle points.\n",
    "___\n",
    "    \n",
    "## <center> Important Calculus Concepts\n",
    "___           \n",
    "### <center> Quotitent Rule:\n",
    "    \n",
    "$$\\frac{\\partial}{\\partial x}(\\frac{f(x)}{g(x)}) = \\frac{f'(x)g(x) - f(x)g'(x)}{[g(x)]^2}$$\n",
    "    \n",
    "___\n",
    "### <center> Exponential Rule:\n",
    "$$y = e^x$$\n",
    "    \n",
    "$$y' = e^x$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# <center> Gradient Descent\n",
    "___\n",
    "#### <center> There is no closed form solution to find the minimum of J(θ). Therefore, we will use an iterative optimization algorithm:\n",
    "\n",
    "$$\\frac{exp^{\\theta_k^Tx}}{\\sum_{j=1}^{K}exp^{\\theta_j^Tx}}$$\n",
    "___    \n",
    "$$f(x) = e^{z}$$\n",
    "___  \n",
    "$$f'(x) = e^{z}$$\n",
    "___\n",
    "$$g(x) = \\sum_{j=1}^{K}e^{z}$$\n",
    "___\n",
    "#### <center> For simplicity we will denote:\n",
    "\n",
    "$$\\sum_{j=1}^{K}e^{z} = \\sum$$\n",
    "\n",
    "$$g'(x) = exp^{z}$$   \n",
    "    \n",
    "___\n",
    "### <center> Using Quotient and Exponential Rule\n",
    "\n",
    "#### <center> When we are dealing with the correctly labeled instance $\\mathbb{1}\\{y^{(i)}=k\\} = 1$.\n",
    "$$\\frac{(e^{z}.\\sum)-e^z.e^z}{\\sum^2}$$\n",
    "    \n",
    "$$= \\frac{e^{z}(\\sum-e^z)}{\\sum^2}$$\n",
    "    \n",
    "___    \n",
    "$$= \\frac{e^{z}}{\\sum}.\\frac{\\sum-e^z}{\\sum}$$\n",
    "    \n",
    "    \n",
    "    \n",
    "$$= \\frac{exp^{\\theta_k^Tx}}{\\sum_{j=1}^{K}exp^{\\theta_j^Tx}}. (1- \\frac{exp^{\\theta_k^Tx}}{\\sum_{j=1}^{K}exp^{\\theta_j^Tx}})$$\n",
    "___   \n",
    "$$= \\hat{y}(1-\\hat{y})$$\n",
    "___\n",
    "#### <center> When we are dealing with the Incorrectly labeled instance $\\mathbb{1}\\{y^{(i)}=k\\} = 0$. \n",
    "    \n",
    "    \n",
    "$$\\frac{(0.\\sum)-e^z.e^z}{\\sum^2}$$\n",
    "    \n",
    "$$= \\frac{-e^z.e^z}{\\sum^2}$$\n",
    "    \n",
    "___    \n",
    "$$= \\frac{-e^{z}}{\\sum}.\\frac{e^z}{\\sum}$$\n",
    "    \n",
    " \n",
    "$$= -\\frac{exp^{\\theta_k^Tx}}{\\sum_{j=1}^{K}exp^{\\theta_j^Tx}}. ( \\frac{exp^{\\theta_k^Tx}}{\\sum_{j=1}^{K}exp^{\\theta_j^Tx}})$$\n",
    "___    \n",
    "$$= -\\hat{y}\\hat{y}$$\n",
    "    \n",
    "___\n",
    "#### <center> We are left with the following form\n",
    "$$ \n",
    "  \\begin{cases}\n",
    "  \\\\ \\hat{y}(1-\\hat{y})\\quad \\text{if} \\quad \\mathbb{1}\\{y^{(i)}=k\\} = 1\\\\\n",
    "  \\\\\n",
    "    -\\hat{y}\\hat{y}    \\quad \\text{   if  } \\quad \\mathbb{1}\\{y^{(i)}=k\\} = 0\\\\\n",
    "    .\n",
    "  \\end{cases}\n",
    "$$\n",
    "___\n",
    "## <center> Final Form\n",
    "$$\\nabla_{\\theta^{k}}J(\\theta) = - \\sum_{i=1}^{m}[x^{(i)}(\\mathbb{1}\\{y^{(i)}=k\\} - P(y^{(i)}=k|x^{(i)};\\theta))]$$\n",
    "\n",
    "$$\\nabla_{\\theta^{k}}J(\\theta) = - \\sum_{i=1}^{m}[x^{(i)}(\\mathbb{1}\\{y^{(i)}=k\\} - \\frac{exp^{\\theta_k^Tx}}{\\sum_{j=1}^{K}exp^{\\theta_j^Tx}})]$$\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# <center> Appendix\n",
    "\n",
    "**Libraries**:\n",
    "```\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn import datasets\n",
    "```\n",
    "**Python Implementation**:\n",
    "```\n",
    "class SoftMaxRegression():\n",
    "    def __init__ (self,n_epochs=50,learning_rate=0.1,tol= 1e-4,penalty =0):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tol = tol\n",
    "        self.penalty = penalty\n",
    "\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        X = np.c_[np.ones((len(X))),X]\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        #convergence check\n",
    "        previous_loss = -float('inf')\n",
    "        self.converged = False\n",
    "\n",
    "        y_mult = self.One_Hot(y)\n",
    "        weight = np.zeros([X.shape[1],len(np.unique(y))])\n",
    "        momentum = weight * 0\n",
    "        self.loss_history = []\n",
    "        self.y_mult = y_mult\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            for i in range(m):\n",
    "                random_index = np.random.randint(m)\n",
    "                xi = X[random_index:random_index+1]\n",
    "                yi = y_mult[random_index:random_index+1]\n",
    "                scores = np.dot(xi,weight)\n",
    "                softmax_prob = self.softmax(scores)\n",
    "                loss = (-1 / m) * np.sum(yi * np.log(softmax_prob)) + (self.penalty/2)*np.sum(np.square(weight))\n",
    "                self.loss_history.append(loss)\n",
    "\n",
    "                gradient = (-1/m) * np.dot(xi.T,(yi-softmax_prob))\n",
    "                gradient[1:] += (self.penalty/m) * weight[1:]\n",
    "                self.learning_rate = self.learning_schedule(epoch * m + i)\n",
    "                momentum = 0.8 * momentum + self.learning_rate * gradient\n",
    "                weight -= momentum\n",
    "\n",
    "        self.weight = weight\n",
    "        return self\n",
    "    \n",
    "    def predict(self,someX):\n",
    "        someX = np.c_[np.ones((len(someX))),someX]\n",
    "        probs = self.softmax(np.dot(someX,self.weight))\n",
    "        preds = np.argmax(probs,axis=1)\n",
    "        self.proability = probs\n",
    "        self.prediction = preds\n",
    "        return probs,preds\n",
    "    \n",
    "    def accuracy(self,Y_test,X_test):\n",
    "        Y_test =  np.unique(Y_test, return_inverse=True)[1]\n",
    "        accuracy = np.sum(X_test[1] == Y_test)/(float(len(Y_test)))\n",
    "        return accuracy\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        z -= np.max(z)\n",
    "        return (np.exp(z).T/np.sum(np.exp(z),axis=1)).T\n",
    "    \n",
    "    def One_Hot(self, y):\n",
    "        Unique,inverse = np.unique(y, return_inverse=True)\n",
    "        self.ohe = np.eye(Unique.shape[0])[inverse]\n",
    "        return self.ohe\n",
    "    \n",
    "    def sigmoid(sef,z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def learning_schedule(self,t,t0 =5, t1 = 50):\n",
    "        return t0 / (t + t1)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
